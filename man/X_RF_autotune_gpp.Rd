% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Xhrf_autotune_gpp.R
\name{X_RF_autotune_gpp}
\alias{X_RF_autotune_gpp}
\title{Gaussian Process optimization for the X-Learner with honest RF for
  both stages}
\usage{
X_RF_autotune_gpp(feat, tr, yobs, ntree = 2000, init_points = 20,
  n_iter = 100, nthread = 0, verbose = TRUE, ...)
}
\arguments{
\item{feat}{A data frame of all the features.}

\item{tr}{A numeric vector contain 0 for control and 1 for treated variables.}

\item{yobs}{A numeric vector containing the observed outcomes.}

\item{ntree}{Number of trees for each of the base learners.}

\item{init_points}{Number of completely randomly selected tuning settings.}

\item{n_iter}{Number of updates updates to optimize the GPP.}

\item{nthread}{Number of threads used. Set it is 0, to automatically select
the maximum amount of possible threads. Set it 1 for slowest performance
but absolute deterministic behavior.}

\item{verbose}{...}

\item{...}{Additional parameters}
}
\value{
A tuned X learner object.
}
\description{
\code{X_RF_autotune_gpp} will first go through 11 example setups
  which have proven to be very good parameters in some cases we have studied
  before. After that 'init_points' many points completely at random and
  evaluates those. After that it uses the previous observations to initialize
  a gaussian process prior and it makes n_iter many updates using this GP
  potimization
}
\details{
This function uses the rBayesianOptimization package to do the
  baysian optimization
}
\examples{
  set.seed(14236142)
  feat <- iris[, -1]
  tr <- rbinom(nrow(iris), 1, .5)
  yobs <- iris[, 1]
  # train a
  xl_gpp <- X_RF_autotune_gpp(feat, tr, yobs, ntree = 100, nthread = 0,
  verbose = FALSE, init_points = 5, n_iter = 1)
  # computes
  EstimateCate(xl_gpp, feat)
  CateCI(xl_gpp, feat, B = 5, verbose = FALSE)

}
\seealso{
\code{\link{X_RF_autotune_simple}},
\code{\link{X_RF_autotune_hyperband}},
}
