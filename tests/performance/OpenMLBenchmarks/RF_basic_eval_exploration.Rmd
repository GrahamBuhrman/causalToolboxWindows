---
title: "Comparing Different versions of Random Forests"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("OpenML")
library("ggplot2")
library("dplyr")
setOMLConfig(apikey = "6e7606dcedb2a6810d88dfaa550f7f07") # https://www.openml.org/u/3454#!api

# read in the data which ran on the cluster:
benchmark_results <- read.csv('~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/MSE_openML_basic.csv')

data_set_statistics <- read.csv("~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/openML_dataset_summary.csv")

# read in the data which was saved on OpenML:
# tasks = listOMLTasks(limit = 100000)
# save(tasks, file = "~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/tasks.Rda")
load("~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/tasks.Rda", verbose = TRUE)
regression_tasks <- tasks[tasks$task.type == "Supervised Regression", ]

task_properties <- regression_tasks %>%
  select(
  "task.id",
  "data.id",
  "name",
  "target.feature",
  "tags",
  "majority.class.size",
  "max.nominal.att.distinct.values",
  "minority.class.size",
  "number.of.classes",
  "number.of.features",
  "number.of.instances",
  'number.of.instances.with.missing.values',
  "number.of.numeric.features",
  "number.of.symbolic.features"
  )
# merge data sets

benchmark_results <- benchmark_results[!duplicated(benchmark_results[ ,c('task.id','estimator')]), ]
if ((!all(table(benchmark_results$task.id) <= 5)) |
    (!all(table(task_properties$task.id) == 1))) {
    warning("Some data is repetitive. We should not merge")
  }

benchmark_cmb_pre <-
  merge(
  task_properties,
  benchmark_results,
  by = c('task.id', 'data.id'),
  all.x = FALSE,
  all.y = TRUE
  )

#max(table(data_set_statistics$data.id))

# get rid of dublicated rows:
data_set_statistics <- data_set_statistics[!duplicated(data_set_statistics), ]

benchmark_cmb <- merge(benchmark_cmb_pre, data_set_statistics, by = 'data.id')

if(nrow(benchmark_cmb) != nrow(benchmark_cmb_pre)){
  # something went wrong here, let's find out what:
  # apperently, we don't have data about the following data sets:
  unique(benchmark_cmb_pre$data.id[!benchmark_cmb_pre$data.id %in% data_set_statistics$data.id])
  
  
  warning('Something went wrong here')
} 



benchmark_cmb %>% tbl_df() %>%
  mutate(MSE = (MSE_1 + MSE_2) / 2) %>%
  select(estimator, MSE, task.id, train_time, predict_time) %>%
  group_by(task.id) %>%
  summarize(MSE_ranger = mean(MSE[estimator == 'ranger']),
            MSE_randomForest = mean(MSE[estimator == 'randomForest']),
            train_time_ranger = mean(train_time[estimator == 'ranger']),
            train_time_randomForest = mean(train_time[estimator == 'randomForest']),
            predict_time_ranger = mean(predict_time[estimator == 'ranger']),
            predict_time_randomForest = mean(predict_time[estimator == 'randomForest'])
            ) -> 
  ave_performance

benchmark_cmb %>% tbl_df() %>%
  mutate(MSE = (MSE_1 + MSE_2) / 2) ->
  benchmark_tmp1

merge(benchmark_tmp1, ave_performance, by = 'task.id') ->
  benchmark_tmp2

if(nrow(benchmark_tmp2) != nrow(benchmark_cmb_pre)) warning('we accidentally created new rows')

# get rid of all the dublicated observations:
benchmark_aggr_all <- benchmark_tmp2[!duplicated(benchmark_tmp2[, c('data.id', 'target.feature', 'estimator')]), ]

# remove all runs for which either algorithm failed:
iscomplete <- benchmark_aggr_all %>% filter(estimator != 'randomForest') %>% group_by(task.id) %>% summarize(complete = !is.na(sum(MSE)))
benchmark_aggr <- benchmark_aggr_all[benchmark_aggr_all$task.id %in% iscomplete$task.id[iscomplete$complete], ]
```


##### TODOs

1. Update everything with the recent runs on SCF.
3. Try to find out where and why we are doing poorly. 
4. The next batch of observations will have several seeds per run, aggregate
the data over those runs to become robust against randomization errors


# Robustness
How often does our algorithm fail, when other algorithms don't fail? 

```{r, echo=FALSE}
did_not_run <- is.na(benchmark_aggr_all$MSE)
robustness_analysis <- table(benchmark_aggr_all$estimator, did_not_run)
robustness_analysis[-4, ]
```

We can see that our algorithm fails in less than 2\% of all cases, but more 
often than ranger. A comparison with randomForest is impossible, since it fails 
in rougly 30\% of all cases, since it does not work for big data sets. 

```{r, echo=FALSE}
did_not_run <- is.na(benchmark_aggr_all$MSE)
robustness_analysis <- table(benchmark_aggr_all$estimator, did_not_run)
robustness_analysis[-4, ]
```

## Why does it sometimes fail?

```{r, echo=FALSE}
benchmark_aggr_all_anp <- benchmark_aggr_all[benchmark_aggr_all$estimator=='hte_adaptive_nomsp', ]

hte_fails <- is.na(benchmark_aggr_all_anp$MSE)
more_features_than_obs <- benchmark_aggr_all_anp$n_features >= benchmark_aggr_all_anp$n_obs

table(
  hte_fails,
  more_features_than_obs
)

```

Conclusion:

* Out of the 22 data sets where our algorithm fails, 21 have more features than
observations. And all data sets ranger fails we fail as well.
* We are still much better than randomForest
* We should reset parameters automatically such that when the parameters are 
choosen too big or too small, that we readjust them such that no error occurs. 
E.g. we often fail because the leaf size is choosen too big. In such a case, 
I think it would be better to set the leaf size to the biggest possible value,
which does not cause an error and throw a warning, or to return just the mean
of all y values.


# MSE Performance

## Performance relative to the explained variance

```{r, echo=FALSE, warning=FALSE, message=FALSE}
benchmark_aggr %>% filter(estimator != 'ranger') %>%
  ggplot(aes(
  x = MSE_ranger / target_var,
  y = (MSE - MSE_ranger) / MSE_ranger,
  color = estimator
  )) +
  geom_point() +
  geom_smooth() +
  coord_cartesian(ylim = c(-1, 1), xlim = NULL) +
  theme_minimal() +
  theme(legend.position = "bottom")

```


## Performance for data set with and without categorical variables

```{r, echo=FALSE, warning=FALSE}
benchmark_aggr %>%
  mutate(
  hasCategoricalFeat = ifelse(
  number.of.features == number.of.numeric.features,
  'no cat feat',
  'categorical features'
  )
  ) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(
  x = estimator,
  y = (MSE - target_var) / target_var
  )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ hasCategoricalFeat) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```

## Performance for data set with high and low signal to noise ratio

```{r, echo=FALSE, warning=FALSE}
stn <- benchmark_aggr$MSE_randomForest / benchmark_aggr$target_var
# if there is a strong sginal, then stn is small

benchmark_aggr %>%
  mutate(
    signalstrength = ifelse(
      benchmark_aggr$MSE_randomForest / benchmark_aggr$target_var < .5,
        'strong signal',
        'weak signal'
      )
    ) %>%
  filter(!is.na(signalstrength)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ signalstrength) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```

## Performance in terms of the size of the data sets

```{r, echo=FALSE, warning=FALSE}
# if there is a strong sginal, then stn is small
benchmark_aggr %>%
  mutate(
    size = ifelse(
      benchmark_aggr$number.of.instances <= 209,
        'tiny',
        ifelse(
          benchmark_aggr$number.of.instances < 500,
          'small',
          'large'
        )
      )
    ) %>%
  filter(!is.na(size)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ size) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```


## Performance in terms of the high and low dimensional

```{r, echo=FALSE, warning=FALSE}
# if there is a strong sginal, then stn is small

feat_obs_ratio <- benchmark_aggr$n_features / benchmark_aggr$n_obs
summary(feat_obs_ratio)

benchmark_aggr %>%
  mutate(
    dimensionality = ifelse(
      feat_obs_ratio < 0.2174 ,
        'low dimensional',
        'high dimensional'
      )
    ) %>%
  filter(!is.na(dimensionality)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ dimensionality) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```


## Performance in terms of many and few features

```{r, echo=FALSE, warning=FALSE}
# if there is a strong sginal, then stn is small

nfeat <- benchmark_aggr$n_features 
summary(nfeat)

benchmark_aggr %>%
  mutate(
    nfeat = ifelse(
      nfeat < 26 ,
        'few features',
        'many features'
      )
    ) %>%
  filter(!is.na(nfeat)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ nfeat) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```




## Run a linear regression to gain intuition for interactions

```{r}

summary(
    lm(
      I(MSE > MSE_ranger) ~ size + 
        size:(hasCategoricalFeat +
              number.of.features  +
              signal_strength +
                feat_obs_ratio),
      data = benchmark_aggr %>% filter(estimator == 'hte_adaptive_wmsp') %>%
        mutate(
          hasCategoricalFeat = number.of.features != number.of.numeric.features,
          signal_strength = 1 - MSE_randomForest / target_var,
          feat_obs_ratio = n_features / n_obs,
          size = ifelse(
            number.of.instances <= 209,
            'tiny',
            ifelse(
              number.of.instances < 500,
              'small',
              'large'
              )))))

# summary(
#     lm(
#       I(MSE > MSE_ranger) ~ I(number.of.features == number.of.numeric.features) +
#         number.of.features +
#         I(MSE_randomForest / target_var),
#       data = benchmark_aggr %>% filter(estimator == 'hte_honest_wmsp')
#     )
#   )

table(benchmark_aggr$MSE[benchmark_aggr$estimator == 'hte_adaptive_wmsp'] > 
      benchmark_aggr$MSE_ranger[benchmark_aggr$estimator == 'hte_adaptive_wmsp']) /
  sum(benchmark_aggr$estimator == 'hte_adaptive_wmsp')

table(benchmark_aggr$MSE[benchmark_aggr$estimator == 'hte_adaptive_wmsp'] > 
      benchmark_aggr$MSE_randomForest[benchmark_aggr$estimator == 'hte_adaptive_wmsp']) /
  sum(benchmark_aggr$estimator == 'hte_adaptive_wmsp')

```

**Conclusion for our adaptive RF:**  

* For the data sets which ran so far, we suffered only marginally, but other 
impelementations did outperform us in 30\% of all cases. The difference for the
MSE however, was rather marginal.
* It seems to be the case that we are not loosing anything in the way we handle 
categorical features. In fact, it seems to be the case that in data sets which 
have categorical features, we are doing slightly better. 
* We are also doing better in settings with a weak signal 
* We somehow loose a lot for small data sets, while we are doing pretty good on
big data sets



## Analyze examples which are particularily bad


```{r, verbose = FALSE}

benchmark_aggr %>% filter(estimator != "ranger") %>%
  filter(n_features < 3 * n_obs) %>% # only look at those with a lot of observations
  filter(n_obs > 300) %>%
  mutate(rel_MSE_to_ranger = (MSE - MSE_ranger) / MSE_ranger) -> benchmark_ranger_vs_hte

benchmark_ranger_vs_hte %>% 
  ggplot(aes(x = estimator, y = rel_MSE_to_ranger)) +
  geom_boxplot() + 
  theme_minimal()

```

#### Analyzing the worst task 2287:

```{r, eval = FALSE}
benchmark_ranger_vs_hte %>% filter(estimator =='hte_adaptive_nomsp',
                                   rel_MSE_to_ranger > .5)
# Let's analyze the worst case:
data.id <- 196
target.feature <- "class"

# -----------------------------------------------------------------------------
data_set <-  getOMLDataSet(data.id = data.id)
non_missing_rows <- apply(!is.na(data_set$data),1, all) # only take rows which
# which don't have missing values

features <- data_set$data[non_missing_rows, colnames(data_set$data) != data_set$target.features]
target <- data_set$data[non_missing_rows,  colnames(data_set$data) == data_set$target.features]


# 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin'
features <- features[, c( 'weight', 'acceleration', 'model')]
# features$model <- as.numeric(features$model)

features$model <- factor(sample(1:100)[as.numeric(features$model)])
# split the data into training and test set
n_smp <- length(target)
# set.seed(7684240)
idx_1 <- sample(1:n_smp, round(n_smp / 2))
idx_2 <- (1:n_smp)[-idx_1]
features_1 <- features[idx_1, ]
features_2 <- features[idx_2, ]
y_1 <- target[idx_1]
y_2  <- target[idx_2]

library(ranger)
library(hte)
ntree = 500
mtry = function(features) max(round(ncol(features) / 3), 1)
nodesize = 5
replace = TRUE
sampsize = function(target) length(target)

ranger_e <- ranger(
              y ~ .,
              data = data.frame(features_1, y = y_1),
              num.trees = ntree,
              mtry = mtry(features_1),
              min.node.size = nodesize,
              replace = replace,
              sample.fraction = 1
              )
(MSE_1 <- mean((y_2 - predict(ranger_e, features_2)$predictions) ^ 2))

hrf_e <- honestRF(
            x = features_1,
            y = y_1,
            ntree = ntree,
            replace = replace,
            sampsize = sampsize(target),
            mtry = mtry(features_1),
            nodesizeSpl = nodesize,
            nodesizeAvg = nodesize,
            splitratio = 1,
            middleSplit = FALSE
          )
(MSE_1 <- mean((y_2 - predict(hrf_e, features_2)) ^ 2))


# ------------------------------------------------------------------------------


```

For task 2287, I discovered a very strange behavior:

* the feature model is a factor containingnumbrs 70, 71, ... 80. In the standard case
ranger is doing much better than we do on average 9.0 and we have 14.0
* If I transform the factor to a numeric (now it has ordering), then we and ranger
do simlarily well and we both roughly get an MSE of 9.0
* If I make the factor random letters i.e. 70 becomse 'i', 71 becomes 'f', then
both methods do similarily poorly and achieve roughly 
* If I randomly reorder the model numbers, but keep them as a factor, i.e. 
70 -> 76, 71 -> 73, ..., then both models seem to achieve roughly around 14.0

##### Conclusion:
I think in this data set ranger is automatically converting a factor of numerics
to a numeric. It is thereby introducing an ordering which can be useful and 
exploited. However, that type of ordering could also be wrong information and 
lead to poorer performance. However, given that in most data sets numerics seem 
to have an ordering, this can be a smar behavior. 

#### Analyzing the worst task 4840:

```{r, eval = FALSE}
benchmark_ranger_vs_hte %>% filter(estimator =='hte_adaptive_nomsp',
                                   rel_MSE_to_ranger > .4)
# Let's analyze the worst case:
data.id <- 196
target.feature <- "velocity"

# -----------------------------------------------------------------------------
data_set <-  getOMLDataSet(data.id = data.id)
non_missing_rows <- apply(!is.na(data_set$data),1, all) # only take rows which
# which don't have missing values

features <- data_set$data[non_missing_rows, colnames(data_set$data) != data_set$target.features]
target <- data_set$data[non_missing_rows,  colnames(data_set$data) == data_set$target.features]

# features$group <- factor(sample(c(letters, LETTERS))[as.numeric(features$group)])

features <- features%>%select( - angle)
YaleToolkit::whatis(data_set$data)

# split the data into training and test set
n_smp <- length(target)
# set.seed(7684240)
idx_1 <- sample(1:n_smp, round(n_smp / 2))
idx_2 <- (1:n_smp)[-idx_1]
features_1 <- features[idx_1, ]
features_2 <- features[idx_2, ]
y_1 <- target[idx_1]
y_2  <- target[idx_2]

library(ranger)
library(hte)
ntree = 500
mtry = function(features) max(round(ncol(features) / 3), 1)
nodesize = 5
replace = TRUE
sampsize = function(target) length(target)

ranger_e <- ranger(
              y ~ .,
              data = data.frame(features_1, y = y_1),
              num.trees = ntree,
              mtry = mtry(features_1),
              min.node.size = nodesize,
              replace = replace,
              sample.fraction = 1
              )
(MSE_1 <- mean((y_2 - predict(ranger_e, features_2)$predictions) ^ 2))

hrf_e <- honestRF(
            x = features_1,
            y = y_1,
            ntree = ntree,
            replace = replace,
            sampsize = sampsize(target),
            mtry = mtry(features_1),
            nodesizeSpl = nodesize,
            nodesizeAvg = nodesize,
            splitratio = 1,
            middleSplit = FALSE
          )
(MSE_1 <- mean((y_2 - predict(hrf_e, features_2)) ^ 2))


# ------------------------------------------------------------------------------


```


# Speed comparison:

## Training time:
```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'randomForest') %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = train_time_ranger, y = train_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'loess') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") 

```

### Training time without randomForest

```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'randomForest') %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = train_time_ranger, y = train_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'loess') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_log10() +
  scale_y_log10()

```



## Prediciton time:
```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = predict_time_ranger, y = predict_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_log10() +
  scale_y_log10()
```


##### Conclusion:

* randomForest is much slower than ranger and our implementation
* It is not clear whether we are slower or faster than ranger. 



# Final Conclusion:

1. It seems to be worth while studying how to automatically exploit missing data.
Most data sets here have missing data
2. It would be nice if our version of rf never fails. 
3. We want to get rid of the warning: "used as adaptive random forest"
4. We seem to be loosing a lot on data sets where some categories are only 
presented in the test data set.