---
title: "Comparing Different versions of Random Forests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("OpenML")
library("ggplot2")
library("dplyr")
setOMLConfig(apikey = "6e7606dcedb2a6810d88dfaa550f7f07") # https://www.openml.org/u/3454#!api

# read in the data which ran on the cluster:
benchmark_results <- read.csv('~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/MSE_openML_basic.csv')

data_set_statistics <- read.csv("~/Dropbox (Personal)/CATE/hte/tests/performance/OpenMLBenchmarks/sim_data/openML_dataset_summary.csv")

# read in the data which was saved on OpenML:
tasks = listOMLTasks()
regression_tasks <- tasks[tasks$task.type == "Supervised Regression", ]

task_properties <- regression_tasks %>%
  select(
  "task.id",
  "data.id",
  "name",
  "target.feature",
  "tags",
  "majority.class.size",
  "max.nominal.att.distinct.values",
  "minority.class.size",
  "number.of.classes",
  "number.of.features",
  "number.of.instances",
  "number.of.numeric.features",
  "number.of.symbolic.features"
  )
# merge data sets


if ((!all(table(benchmark_results$task.id) <= 5)) |
    (!all(table(task_properties$task.id) == 1))) {
    stop("Some data is repetitive. We should not merge")
  }

benchmark_cmb_pre <-
  merge(
  task_properties,
  benchmark_results,
  by = c('task.id', 'data.id'),
  all.x = FALSE,
  all.y = TRUE
  )

table(data_set_statistics$data.id)

# get rid of dublicated rows:
data_set_statistics <- data_set_statistics[!duplicated(data_set_statistics), ]

benchmark_cmb <- merge(benchmark_cmb_pre, data_set_statistics, by = 'data.id')

if(nrow(benchmark_cmb) != nrow(benchmark_cmb_pre)) stop('we accidentally created new rows')

benchmark_cmb %>% tbl_df() %>%
  mutate(MSE = (MSE_1 + MSE_2) / 2) %>%
  select(estimator, MSE, task.id, train_time, predict_time) %>%
  group_by(task.id) %>%
  summarize(MSE_ranger = mean(MSE[estimator == 'ranger']),
            MSE_randomForest = mean(MSE[estimator == 'randomForest']),
            train_time_ranger = mean(train_time[estimator == 'ranger']),
            train_time_randomForest = mean(train_time[estimator == 'randomForest']),
            predict_time_ranger = mean(predict_time[estimator == 'ranger']),
            predict_time_randomForest = mean(predict_time[estimator == 'randomForest'])
            ) -> 
  ave_performance

benchmark_cmb %>% tbl_df() %>%
  mutate(MSE = (MSE_1 + MSE_2) / 2) ->
  benchmark_tmp1

merge(benchmark_tmp1, ave_performance, by = 'task.id') ->
  benchmark_tmp2

if(nrow(benchmark_tmp2) != nrow(benchmark_cmb_pre)) stop('we accidentally created new rows')

# get rid of all the dublicated observations:
benchmark_aggr_all <- benchmark_tmp2[!duplicated(benchmark_tmp2[, c('data.id', 'target.feature', 'estimator')]), ]

# remove all runs for which either algorithm failed:
iscomplete <- benchmark_aggr_all %>% group_by(task.id) %>% summarize(complete = !is.na(sum(MSE)))
benchmark_aggr <- benchmark_aggr_all[benchmark_aggr_all$task.id %in% iscomplete$task.id[iscomplete$complete], ]
```


##### TODOs

1. Update everything with the recent runs on SCF
2. Try to get data from that fraf formate
3. Analyze the outliers and try to nail down where we are loosing
4. Try to find out why we are doing poorly. Is it because ranger is and 
randomForest is pooling?


# MSE Performance

## Performance relative to the explained variance

```{r, echo=FALSE, warning=FALSE, message=FALSE}
benchmark_aggr %>% filter(estimator != 'ranger') %>%
  ggplot(aes(
  x = MSE_ranger / target_var,
  y = (MSE - MSE_ranger) / MSE_ranger,
  color = estimator
  )) +
  geom_point() +
  geom_smooth() +
  # coord_cartesian(ylim = c(-1, 1), xlim = c(0, 1.6)) +
  theme_minimal() +
  theme(legend.position = "bottom")

```


## Performance for data set with and without categorical variables

```{r, echo=FALSE, warning=FALSE}
benchmark_aggr %>%
  mutate(
  hasCategoricalFeat = ifelse(
  number.of.features == number.of.numeric.features,
  'no cat feat',
  'categorical features'
  )
  ) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(
  x = estimator,
  y = (MSE - target_var) / target_var
  )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ hasCategoricalFeat) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```

## Performance for data set with high and low signal to noise ratio

```{r, echo=FALSE, warning=FALSE}
stn <- benchmark_aggr$MSE_randomForest / benchmark_aggr$target_var
# if there is a strong sginal, then stn is small

benchmark_aggr %>%
  mutate(
    signalstrength = ifelse(
      benchmark_aggr$MSE_randomForest / benchmark_aggr$target_var < .5,
        'strong signal',
        'weak signal'
      )
    ) %>%
  filter(!is.na(signalstrength)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ signalstrength) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```

## Performance in terms of the size of the data sets

```{r, echo=FALSE, warning=FALSE}
# if there is a strong sginal, then stn is small
benchmark_aggr %>%
  mutate(
    size = ifelse(
      benchmark_aggr$number.of.instances <= 209,
        'tiny',
        ifelse(
          benchmark_aggr$number.of.instances < 500,
          'small',
          'large'
        )
      )
    ) %>%
  filter(!is.na(size)) %>%
  filter(estimator != 'hte_adaptive_nomsp') %>%
  ggplot(aes(x = estimator, y = (MSE - target_var) / target_var )) +
  geom_boxplot(alpha = .5) +
  theme_minimal() +
  facet_grid(. ~ size) +
  theme(legend.position = "bottom", axis.text.x=element_text(angle = 90, hjust = 0))

```

## Run a linear regression to find which data sets are bed for 

```{r}

summary(
    lm(
      I(MSE > MSE_ranger) ~ size + 
        size:(hasCategoricalFeat +
              number.of.features  +
              signal_strength),
      data = benchmark_aggr %>% filter(estimator == 'hte_adaptive_wmsp') %>%
        mutate(
          hasCategoricalFeat = number.of.features != number.of.numeric.features,
          signal_strength = 1 - MSE_randomForest / target_var,
          size = ifelse(
            number.of.instances <= 209,
            'tiny',
            ifelse(
              number.of.instances < 500,
              'small',
              'large'
              )))))

# summary(
#     lm(
#       I(MSE > MSE_ranger) ~ I(number.of.features == number.of.numeric.features) +
#         number.of.features +
#         I(MSE_randomForest / target_var),
#       data = benchmark_aggr %>% filter(estimator == 'hte_honest_wmsp')
#     )
#   )

table(benchmark_aggr$MSE[benchmark_aggr$estimator == 'hte_adaptive_wmsp'] > 
      benchmark_aggr$MSE_ranger[benchmark_aggr$estimator == 'hte_adaptive_wmsp']) /
  sum(benchmark_aggr$estimator == 'hte_adaptive_wmsp')

table(benchmark_aggr$MSE[benchmark_aggr$estimator == 'hte_adaptive_wmsp'] > 
      benchmark_aggr$MSE_randomForest[benchmark_aggr$estimator == 'hte_adaptive_wmsp']) /
  sum(benchmark_aggr$estimator == 'hte_adaptive_wmsp')

```

**Conclusion for our adaptive RF:**  

* For the data sets which ran so far, we suffered only marginally, but other 
impelementations did outperform us in 30\% of all cases. The difference for the
MSE however, was rather marginal.
* It seems to be the case that we are not loosing anything in the way we handle 
categorical features. In fact, it seems to be the case that in data sets which 
have categorical features, we are doing slightly better. 
* We are also doing better in settings with a weak signal 
* We somehow loose a lot for small data sets, while we are doing pretty good on
big data sets


# Speed comparison:

## Training time:
```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = train_time_ranger, y = train_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") 


```

### Training time without randomForest

```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'randomForest') %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = train_time_ranger, y = train_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'loess') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_log10() +
  scale_y_log10()

```



## Prediciton time:
```{r, echo=FALSE, warning=FALSE, message=FALSE}

benchmark_aggr %>%
  filter(estimator != 'ranger') %>%
  ggplot(aes(x = predict_time_ranger, y = predict_time, color = estimator )) +
  geom_point() +
  geom_smooth(method = 'lm') + 
  geom_abline(slope = 1) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_x_log10() +
  scale_y_log10()
```


##### Conclusion:

* randomForest is much slower than ranger and our implementation
* It is not clear whether we are slower or faster than ranger. 

