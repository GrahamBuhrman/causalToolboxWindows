---
title: "User Manual"
author: "Ling Xie"
date: "12/4/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Getting Started

1. Load an example dataset 
In the following, we load an example dataset named "gotv" and then randomly split the dataset into a training and a test set. Extract features, treatment assignment and outcomes from the data. 

Note: The treatment assignment has to be either 0 (control) or 1 (treatment). 

```{r}
devtools::load_all()
data <- get(load('~/Dropbox/01d-data_files.RData'))
data$treatment[data$treatment == 2] <- 0
gotv <- data[data$treatment == 0 | data$treatment == 1, ]
set.seed(2345)
sample_size <- floor(0.8 * nrow(gotv))
train_index <-  sample(seq_len(nrow(gotv)), size = sample_size)
train <- gotv[train_index, ]
train <- train[1:5000, ]
test <- gotv[-train_index, ]
test <- test[1:1000, ]

feature_tr <- train[,1:7]
w_tr <- train$treatment
yobs_tr <- train$voted

feature_test <- test[,1:7]
```

2. Create a learner object
Now that we have our features and treatment assignments, we can try to estimate the conditional expectations of the outcomes of units in control and treatment group separately and then take their differences. Let's start with a T-learner. We can create a T-learner instance with a given training set based on two algorithms, which are Random Forests (T_RF) and Bayesian Additive Regression Trees (T_BART). 

When using T_BART, we can chooes either "dbarts" or "BayesTree" package to implement the base algorithm. To speed up the computation, we need to define the number of threads that would be used for parallel computing when initializing an learner instance.
```{r}
t_rf <- T_RF(feat = feature_tr, tr = w_tr, yobs = yobs_tr)
t_bart_db_1 <- T_BART(feat = feature_tr, tr = w_tr, yobs = yobs_tr, 
                 tree_package = "dbarts",
                 nthread = 1)
t_bart_db_4 <- T_BART(feat = feature_tr, tr = w_tr, yobs = yobs_tr, 
                 tree_package = "dbarts",
                 nthread = 4)
```

3. Estimate the CATE
Then we can estimate the CATE by passing our T-learner instance and test features to EstimateCate method. 
```{r}
cate_esti_rf <- EstimateCate(t_rf, feature_test)
cate_esti_bart_db <- EstimateCate(t_bart_db_4, feature_test)
```

The following is the graph showing the time it takes for each function. 
```{r}
library(microbenchmark)
benchmark <- microbenchmark(
    EstimateCate(t_rf, feature_test),
    EstimateCate(t_bart_db_1, feature_test),
    EstimateCate(t_bart_db_4, feature_test),
    times = 20
)
ggplot2::autoplot(benchmark) 
```


4. Evaluate the performance
We can evaluate the performance of our learner by calculating its mean squared error from the true outcomes. 
```{r}
cate_true <- test$voted
mean((cate_esti_rf - cate_true) ^ 2)
mean((cate_esti_bart_db - cate_true) ^ 2)
```

5. Calculate the Confidence Interval
The CateCI method construacts confidence intervals for the CATE via bootstrapping. 
```{r, eval=FALSE}
t_ci_rf <- CateCI(t_rf, feature_test, B = 500)
t_ci_dbart <- CateCI(t_bart_db, feature_test, B = 500)
```

